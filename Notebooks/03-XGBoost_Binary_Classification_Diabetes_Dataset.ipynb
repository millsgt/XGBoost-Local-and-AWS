{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "782a07bf-08de-4030-88e1-6731c4ac956e"
    }
   },
   "source": [
    "## Diabetes dataset \n",
    "### Predict if a person is at risk of developing diabetes\n",
    "\n",
    "### This Dataset is Freely Available\n",
    "\n",
    "### Overview:\n",
    "The data was collected and made available by the \"National Institute of Diabetes and Digestive and Kidney Diseases\" as part of the Pima Indians Diabetes Database. \n",
    "\n",
    "`Diabetes.csv` is available [from Kaggle](https://www.kaggle.com/uciml/pima-indians-diabetes-database). We have several questions - what information is more correlated with a positive diagnosis, and if we can only ask two questions to a patient, what should we ask and how would we give them a risk of being diagnosed.\n",
    "\n",
    "++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "The following features have been provided to help us predict whether a person is diabetic or not:\n",
    "* **Pregnancies:**  Number of times pregnant\n",
    "* **Glucose:** Plasma glucose concentration over 2 hours in an oral glucose tolerance test\n",
    "* **BloodPressure:** Diastolic blood pressure (mm Hg)\n",
    "* **SkinThickness:** Triceps skin fold thickness (mm)\n",
    "* **Insulin:** 2-Hour serum insulin (mu U/ml)\n",
    "* **BMI:** Body mass index (weight in kg/(height in m)2)\n",
    "* **DiabetesPedigreeFunction:** Diabetes pedigree function (a function which scores likelihood of diabetes based on family history)\n",
    "* **Age:** Age (years)\n",
    "* **Outcome:** Class variable (0 if non-diabetic, 1 if diabetic)\n",
    "\n",
    "### Binary Classification problem - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "6c6a8672-d428-410a-82fa-7f587c9ef2ae"
    }
   },
   "outputs": [],
   "source": [
    "# Install xgboost in notebook instance.\n",
    "#### Command to install xgboost\n",
    "#!pip install xgboost==0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "652b58d4-3b75-405f-9f11-24d0cd1f9656"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "a3946273-d086-4564-b0f1-6adc225191c3"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../Data/Diabetes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## only keep rows where non of the columns has 0 value (except the first and last columns)\n",
    "data = data[~(data[data.columns[1:-1]] == 0).any(axis=1)]\n",
    "data.reset_index(inplace=True, drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using isnull() function  \n",
    "# print(data.isnull().any().sum())\n",
    "print(data.isnull().sum())\n",
    "#data.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['Insulin'], inplace = True)\n",
    "data.reset_index(inplace=True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Replace missing values in each column with the mean or median of that column\n",
    "#data.fillna(data.mean())\n",
    "data.fillna(data.median(), inplace=True)\n",
    "\n",
    "### Drop all rows that contain missing values?\n",
    "#data = data.dropna()\n",
    "#data.reset_index(inplace=True, drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training = 70% of the data\n",
    "# Validation = 30% of the data\n",
    "# Randomize the datset\n",
    "np.random.seed(5)\n",
    "l = list(data.index)\n",
    "np.random.shuffle(l)\n",
    "data = data.iloc[l]\n",
    "data.reset_index(inplace=True, drop = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = data.shape[0]\n",
    "train = int(.7 * rows)\n",
    "test = rows - train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Set\n",
    "df_train = data[:train]\n",
    "#df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation Set\n",
    "df_validation = data[train:]\n",
    "#df_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "a195ae30-1962-4427-859b-73a013dc10d6"
    }
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "377 * 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "e30e8aeb-1ca2-4851-bc2d-1bdee29ab1cf"
    }
   },
   "outputs": [],
   "source": [
    "df_validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "3b240613-803d-4fa9-93cf-53ef68df7b93"
    }
   },
   "outputs": [],
   "source": [
    "X_train = df_train.iloc[:,:-1] # Features: all columns excep last\n",
    "y_train = df_train.iloc[:,-1].ravel() # Target: last column\n",
    "\n",
    "X_validation = df_validation.iloc[:,:-1]\n",
    "y_validation = df_validation.iloc[:,-1].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "9edc89e7-45d3-4350-9eb4-3e0938c3c55e"
    }
   },
   "outputs": [],
   "source": [
    "# Launch a classifier\n",
    "# XGBoost Training Parameter Reference: \n",
    "#   https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "classifier = xgb.XGBClassifier (objective=\"binary:logistic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "348296fb-8c9b-4598-ad2e-d1fe8e10f76a"
    }
   },
   "outputs": [],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "9839d7ce-e791-4d93-bc5f-28604ffde022"
    }
   },
   "outputs": [],
   "source": [
    "classifier.fit(X_train,\n",
    "               y_train, \n",
    "               eval_set = [(X_train, y_train), (X_validation, y_validation)], \n",
    "               eval_metric=['logloss'],\n",
    "               early_stopping_rounds=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "e08f22c1-4346-4e2d-96a2-9974ed5c59ff"
    }
   },
   "outputs": [],
   "source": [
    "eval_result = classifier.evals_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "092776c3-a611-4f40-91e2-664b3b99d05e"
    }
   },
   "outputs": [],
   "source": [
    "training_rounds = range(len(eval_result['validation_0']['logloss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "2e9af3f7-fb85-4c52-83d5-ff9cae457294"
    }
   },
   "outputs": [],
   "source": [
    "print(training_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "5e71239a-e321-43ba-ac2c-993b57b3be3a"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(x=training_rounds,y=eval_result['validation_0']['logloss'],label='Training Error')\n",
    "plt.scatter(x=training_rounds,y=eval_result['validation_1']['logloss'],label='Validation Error')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('LogLoss')\n",
    "plt.title('Training Vs Validation Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notice:\n",
    "* Model is not generalising well, low train error but high validation error\n",
    "* Model has high variance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "f144f315-6d38-429e-8c17-06c17a446198"
    }
   },
   "outputs": [],
   "source": [
    "xgb.plot_importance(classifier)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3312675d-307c-4eff-b835-34f0e7f57924"
    }
   },
   "source": [
    "#### Predict the Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "9b5cb70d-6069-4511-810e-fd17e72667dd"
    }
   },
   "outputs": [],
   "source": [
    "X_test = df_validation.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "f611c852-50e3-4a1a-9134-c1c6e82ad780"
    }
   },
   "outputs": [],
   "source": [
    "result = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "2c573c2b-4143-4e01-b107-e6b871ce0249"
    }
   },
   "outputs": [],
   "source": [
    "df_validation['predicted_class'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "5ad0fa04-6896-46b5-bc23-40d61480d7ca"
    }
   },
   "outputs": [],
   "source": [
    "df_validation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classifier Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "# Explicitly stating labels. Pass=1, Fail=0\n",
    "def true_positive(y_true, y_pred): \n",
    "    return confusion_matrix(y_true, y_pred,labels=[1,0])[0, 0]\n",
    "\n",
    "def true_negative(y_true, y_pred): \n",
    "    return confusion_matrix(y_true,y_pred,labels=[1,0])[1, 1]\n",
    "\n",
    "def false_positive(y_true, y_pred): \n",
    "    return confusion_matrix(y_true, y_pred,labels=[1,0])[1, 0]\n",
    "\n",
    "def false_negative(y_true, y_pred): \n",
    "    return confusion_matrix(y_true, y_pred,labels=[1,0])[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Binary Classifier Metrics\n",
    "# Returns a dictionary {\"MetricName\":Value,...}\n",
    "\n",
    "def binary_classifier_metrics(y_true, y_pred):\n",
    "    metrics = {}\n",
    "\n",
    "    # References: \n",
    "    #  https://docs.aws.amazon.com/machine-learning/latest/dg/binary-classification.html\n",
    "    #  https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "    \n",
    "    # Definition:\n",
    "    # true positive = tp = how many samples were correctly classified as positive (count)\n",
    "    # true negative = tn = how many samples were correctly classified as negative (count)\n",
    "    # false positive = fp = how many negative samples were mis-classified as positive (count)\n",
    "    # false_negative = fn = how many positive samples were mis-classified as negative (count)\n",
    "    \n",
    "    # positive = number of positive samples (count)\n",
    "    #          = true positive + false negative\n",
    "    # negative = number of negative samples (count)\n",
    "    #          = true negative + false positive\n",
    "    \n",
    "    tp = true_positive(y_true, y_pred)\n",
    "    tn = true_negative(y_true, y_pred)\n",
    "    fp = false_positive(y_true, y_pred)\n",
    "    fn = false_negative(y_true, y_pred)\n",
    "    \n",
    "    positive = tp + fn\n",
    "    negative = tn + fp\n",
    "    \n",
    "    metrics['TruePositive'] = tp\n",
    "    metrics['TrueNegative'] = tn\n",
    "    metrics['FalsePositive'] = fp\n",
    "    metrics['FalseNegative'] = fn\n",
    "    \n",
    "    metrics['Positive'] = positive\n",
    "    metrics['Negative'] = negative\n",
    "    \n",
    "    # True Positive Rate (TPR, Recall) = true positive/positive\n",
    "    # How many positives were correctly classified? (fraction)\n",
    "    # Recall value closer to 1 is better. closer to 0 is worse\n",
    "    if tp == 0:\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = tp/positive\n",
    "        \n",
    "    metrics['Recall'] = recall\n",
    "    \n",
    "    # True Negative Rate = True Negative/negative\n",
    "    # How many negatives were correctly classified? (fraction)\n",
    "    # True Negative Rate value closer to 1 is better. closer to 0 is worse\n",
    "    if tn == 0:\n",
    "        tnr = 0\n",
    "    else:\n",
    "        tnr = tn/(negative)\n",
    "    metrics['TrueNegativeRate'] = tnr\n",
    "    \n",
    "    # Precision = True Positive/(True Positive + False Positive)\n",
    "    # How many positives classified by the algorithm are really positives? (fraction)\n",
    "    # Precision value closer to 1 is better. closer to 0 is worse\n",
    "    if tp == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = tp/(tp + fp)\n",
    "    metrics['Precision'] = precision\n",
    "    \n",
    "    # Accuracy = (True Positive + True Negative)/(total positive + total negative)\n",
    "    # How many positives and negatives were correctly classified? (fraction)\n",
    "    # Accuracy value closer to 1 is better. closer to 0 is worse\n",
    "    accuracy = (tp + tn)/(positive + negative)\n",
    "    metrics['Accuracy'] = accuracy\n",
    "    \n",
    "    # False Positive Rate (FPR, False Alarm) = False Positive/(total negative)\n",
    "    # How many negatives were mis-classified as positives (fraction)\n",
    "    # False Positive Rate value closer to 0 is better. closer to 1 is worse\n",
    "    if fp == 0:\n",
    "        fpr = 0\n",
    "    else:\n",
    "        fpr = fp/(negative)\n",
    "    metrics['FalsePositiveRate'] = fpr\n",
    "    \n",
    "    # False Negative Rate (FNR, Misses) = False Negative/(total Positive)\n",
    "    # How many positives were mis-classified as negative (fraction)\n",
    "    # False Negative Rate value closer to 0 is better. closer to 1 is worse\n",
    "    fnr = fn/(positive)\n",
    "    metrics['FalseNegativeRate'] = fnr\n",
    "    \n",
    "    # F1 Score = harmonic mean of Precision and Recall\n",
    "    # F1 Score closer to 1 is better. Closer to 0 is worse.\n",
    "    if precision == 0 or recall == 0:\n",
    "        f1 = 0\n",
    "    else:        \n",
    "        f1 = 2*precision*recall/(precision+recall)\n",
    "\n",
    "    metrics['F1'] = f1\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: \n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    #else:\n",
    "    #    print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(df_validation['Outcome'], df_validation['predicted_class'],labels=[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['Diabetic','Normal'],\n",
    "                      title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['Diabetic','Normal'],\n",
    "                      title='Confusion Matrix - Fraction', normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [binary_classifier_metrics(df_validation['Outcome'], df_validation['predicted_class'])]\n",
    "df_metrics=pd.DataFrame.from_dict(metrics)\n",
    "df_metrics.index = ['Model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Counts')\n",
    "print(df_metrics[['TruePositive',\n",
    "                  'FalseNegative',\n",
    "                  'FalsePositive',\n",
    "                  'TrueNegative',]].round(2))\n",
    "print()\n",
    "print('Fractions')\n",
    "print(df_metrics[['Recall',\n",
    "                  'FalseNegativeRate',\n",
    "                  'FalsePositiveRate',\n",
    "                  'TrueNegativeRate',]].round(2))\n",
    "print()\n",
    "\n",
    "print(df_metrics[['Precision',\n",
    "                  'Accuracy',\n",
    "                  'F1']].round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(\n",
    "    df_validation['Outcome'],\n",
    "    df_validation['predicted_class'],\n",
    "    labels=[1,0],\n",
    "    target_names=['Diabetic','Normal']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Performance not Good Enough?\n",
    "#### Debug your Data before you debug your Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

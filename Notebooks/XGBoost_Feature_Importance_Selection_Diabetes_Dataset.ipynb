{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "782a07bf-08de-4030-88e1-6731c4ac956e"
    }
   },
   "source": [
    "## Diabetes dataset \n",
    "### Predict if a person is at risk of developing diabetes\n",
    "\n",
    "### This Dataset is Freely Available\n",
    "\n",
    "### Overview:\n",
    "The data was collected and made available by the \"National Institute of Diabetes and Digestive and Kidney Diseases\" as part of the Pima Indians Diabetes Database. \n",
    "\n",
    "`Diabetes.csv` is available [from Kaggle](https://www.kaggle.com/uciml/pima-indians-diabetes-database). We have several questions - what information is more correlated with a positive diagnosis, and if we can only ask two questions to a patient, what should we ask and how would we give them a risk of being diagnosed.\n",
    "\n",
    "++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "The following features have been provided to help us predict whether a person is diabetic or not:\n",
    "* **Pregnancies:**  Number of times pregnant\n",
    "* **Glucose:** Plasma glucose concentration over 2 hours in an oral glucose tolerance test\n",
    "* **BloodPressure:** Diastolic blood pressure (mm Hg)\n",
    "* **SkinThickness:** Triceps skin fold thickness (mm)\n",
    "* **Insulin:** 2-Hour serum insulin (mu U/ml)\n",
    "* **BMI:** Body mass index (weight in kg/(height in m)2)\n",
    "* **DiabetesPedigreeFunction:** Diabetes pedigree function (a function which scores likelihood of diabetes based on family history)\n",
    "* **Age:** Age (years)\n",
    "* **Outcome:** Class variable (0 if non-diabetic, 1 if diabetic)\n",
    "\n",
    "### Binary Classification problem - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "6c6a8672-d428-410a-82fa-7f587c9ef2ae"
    }
   },
   "outputs": [],
   "source": [
    "# Install xgboost in notebook instance.\n",
    "#### Command to install xgboost\n",
    "#!pip install xgboost==0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "652b58d4-3b75-405f-9f11-24d0cd1f9656"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import plot_importance\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "a3946273-d086-4564-b0f1-6adc225191c3"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../Data/Diabetes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## only keep rows where non of the columns has 0 value (except the first and last columns)\n",
    "data = data[~(data[data.columns[1:-1]] == 0).any(axis=1)]\n",
    "data.reset_index(inplace=True, drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using isnull() function  \n",
    "# print(data.isnull().any().sum())\n",
    "print(data.isnull().sum())\n",
    "#data.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['Insulin'], inplace = True)\n",
    "data.reset_index(inplace=True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Replace missing values in each column with the mean or median of that column\n",
    "#data.fillna(data.mean())\n",
    "data.fillna(data.median(), inplace=True)\n",
    "\n",
    "### Drop all rows that contain missing values?\n",
    "#data = data.dropna()\n",
    "#data.reset_index(inplace=True, drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,:-1] # Features: all columns excep last\n",
    "y = data.iloc[:,-1].ravel() # Target: last column\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch XGBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "9edc89e7-45d3-4350-9eb4-3e0938c3c55e"
    }
   },
   "outputs": [],
   "source": [
    "# Launch a classifier\n",
    "# XGBoost Training Parameter Reference: \n",
    "#   https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "classifier = xgb.XGBClassifier (objective=\"binary:logistic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "348296fb-8c9b-4598-ad2e-d1fe8e10f76a"
    }
   },
   "outputs": [],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "9839d7ce-e791-4d93-bc5f-28604ffde022"
    }
   },
   "outputs": [],
   "source": [
    "classifier.fit(X_train,\n",
    "               y_train, \n",
    "               eval_metric=['logloss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "e08f22c1-4346-4e2d-96a2-9974ed5c59ff"
    }
   },
   "outputs": [],
   "source": [
    "# plot feature importance\n",
    "plot_importance(classifier)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3312675d-307c-4eff-b835-34f0e7f57924"
    }
   },
   "source": [
    "### Feature Selection using Feature Importance\n",
    "* Feature importance scores can be used for feature selection in scikit-learn.\n",
    "* This is done using the SelectFromModel class that takes a model and can transform a dataset into a subset with selected features.\n",
    "* This class can take a pre-trained model, such as one trained on the entire training dataset. \n",
    "* It can then use a threshold to decide which features to select. \n",
    "* This threshold is used when you call the transform() method on the SelectFromModel instance to consistently select the same features on the training dataset and the test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "9b5cb70d-6069-4511-810e-fd17e72667dd"
    }
   },
   "outputs": [],
   "source": [
    "# fit model on all training data\n",
    "model = xgb.XGBClassifier(objective=\"binary:logistic\", use_label_encoder =False)\n",
    "model.fit(X_train, y_train, eval_metric=['logloss'])\n",
    "# make predictions for test data and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "# Fit model using each importance as a threshold\n",
    "thresholds = np.sort(model.feature_importances_)\n",
    "for thresh in thresholds:\n",
    "    # select features using threshold\n",
    "    selection = SelectFromModel(model, threshold=thresh, prefit=True)\n",
    "    select_X_train = selection.transform(X_train)\n",
    "    # train model\n",
    "    selection_model = xgb.XGBClassifier(objective=\"binary:logistic\", use_label_encoder =False)\n",
    "    selection_model.fit(select_X_train, y_train, eval_metric=['logloss'])\n",
    "    # eval model\n",
    "    select_X_test = selection.transform(X_test)\n",
    "    y_pred = selection_model.predict(select_X_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1], accuracy*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f611c852-50e3-4a1a-9134-c1c6e82ad780"
    }
   },
   "source": [
    "You can see that the performance of the model generally decreases with the number of selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
